![](_page_0_Picture_0.jpeg)

# **Advanced Prompt Engineering for Complex Multi‑Step Tasks (2025)**

**Abstract:** Prompt engineering has become a critical skill for leveraging large language models (LLMs) in solving complex, multi-step tasks. This paper provides a comprehensive guide for AI agents on crafting effective prompts that enable accurate **planning**, **execution**, and **verification** of intricate tasks. We synthesize the latest research (as of August 2025) on multi-step reasoning, tool-augmented prompting, selfcorrection strategies, and domain-specific techniques. The aim is to instruct an AI agent (with tool use capabilities like web search, code execution, etc.) in becoming a "master prompt engineer" – able to autonomously plan solutions, take actions methodically, and verify outcomes for tasks ranging from software coding and document composition to creative writing and complex problem solving.

## **Introduction**

In recent years, large language models have evolved from simple Q&A systems into **autonomous agents** capable of handling elaborate, multi-step operations. Modern models (e.g. GPT-4 variants, Claude 4, Google Gemini) not only generate text but can also call tools, execute code, and interact with environments to accomplish user goals . This new capability landscape demands **advanced prompt engineering** – designing prompts that guide an AI agent through **planning, execution, and verification** steps needed for complex tasks. [1](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Prompt%20engineering%20isn%E2%80%99t%20just%20a,systems%20useful%2C%20reliable%2C%20and%20safe) [2](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=In%202023%2C%20you%20could%20get,assignments%2C%20and%20even%20adversarial%20exploits)

Prompt engineering is *"the practice of crafting inputs—called prompts—to get the best possible results from a large language model"* . Unlike traditional programming with strict code, prompting is done in natural language, making it a subtle art that directly impacts the **usefulness, reliability, and safety** of AI outputs . When tasks involve multiple interdependent steps (e.g. writing software code and then testing it, or gathering information via web search and then composing a report), a single-shot instruction often falls short. Instead, the agent must be guided to **think, act, and check** its work in stages. Recent research emphasizes structured approaches – such as chain-of-thought reasoning, plan verification, and selfrefinement – to tackle such complexity . [3](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=What%20Is%20Prompt%20Engineering%3F) [4](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=In%20simple%20terms%2C%20prompt%20engineering,a%20way%20it%20truly%20understands) [5](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem) [6](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20Chain,reducing%20the%20occurrence%20of%20hallucinations)

This paper serves as both **reference documentation and practical instruction** for an AI agent to excel in prompt engineering. We will cover fundamental principles for prompt clarity and structure, then delve into advanced techniques for multi-step reasoning and tool use. Key frameworks like **Chain-of-Thought (CoT)** prompting, the **ReAct** (Reason+Act) paradigm, **Chain-of-Verification (CoVe)**, and **Reflexion** (self-reflection) are explained with up-to-date findings. We provide domain-specific guidance in areas including software engineering, document and creative writing, analytical problem solving, and file system tasks – illustrating how an AI agent can craft prompts to reliably plan, execute, and verify its actions in each domain.

## **Foundations of Effective Prompt Design**

Prompt engineering starts with strong fundamentals: **clarity, context, and structure**. Many prompt failures stem not from model limitations but from ambiguity or poor prompt design . Before tackling complex workflows, an AI agent should master these foundational practices: [7](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=,thinner%20than%20most%20people%20think)

- **Be Clear and Specific:** Ambiguity is the enemy. A prompt should explicitly state the task, desired output format, and any constraints. For example, *instead of* a vague request like *"Write a summary."*, an effective prompt specifies: *"Summarize the following customer support chat in three bullet points, focusing on the issue, customer sentiment, and resolution. Use clear, concise language."* . The latter leaves no room for interpretation, resulting in more consistent outputs. Always prefer precise instructions over open-ended ones; studies show that **unambiguous prompts** lead to more reliable and repeatable results . • [8](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=A%20Quick%20Example) [9](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,these%20two%20requests) [10](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=When%20providing%20multiple%20instructions%2C%20remember,your%20instructions%20for%20best%20results)
- **Provide Sufficient Context:** If the task involves external knowledge or prior information (e.g. a document to summarize or codebase to modify), include the relevant context in the prompt. Clearly delineate the context from instructions, for instance by saying *"Here is the background info: … Now based on this, do X."* When multiple documents or data sources are involved, use consistent formatting or metadata tags to help the model distinguish them . For example, wrapping content in tags like <doc id=1 title="..."> ... </doc> creates clear boundaries between sources, making it easier for the model to reference specific pieces . This is increasingly important as LLM context windows grow (now up to millions of tokens) – dumping raw text is less effective than structuring it logically . • [11](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=to%20use%20its%20general%20knowledge,your%20provided%20information%20is%20incomplete) [11](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=to%20use%20its%20general%20knowledge,your%20provided%20information%20is%20incomplete) [12](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Managing%20Long%20Context%20Windows%3A%20Working,with%20Extensive%20Information) [13](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Strict%20Context%20Adherence%3A%20,your%20provided%20information%20is%20incomplete)
- **Use Structured Formatting:** Leverage formatting (Markdown or XML-style tags) *within* the prompt to organize information. A well-structured prompt is easier for the model to parse and follow . For instance, use headings, bullet points, or numbered steps to outline the task procedure. You might explicitly enumerate requirements or steps as part of the prompt. Research and practice in 2025 highlight that models pay special attention to the **beginning and end** of the prompt (primacy and recency effects) . Thus, crucial instructions should appear at those positions (e.g. a final "Important: Do not proceed until all steps above are done" at the end to reinforce a constraint ). An example prompt structure for complex tasks is: • [14](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Just%20as%20a%20well,understand%20and%20process%20your%20request) [15](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=When%20working%20with%20long%20prompts%2C,ensure%20they%20receive%20adequate%20attention) [10](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=When%20providing%20multiple%20instructions%2C%20remember,your%20instructions%20for%20best%20results)
- **Role Definition & Objective:** e.g. *"You are a cybersecurity analyst specialized in incident response. Your goal is to …"* •
- **Core Instructions:** detailed guidance on the task (*what to do* and *what* not *to do*). •
- **Reasoning Framework:** directions on how to approach the problem (e.g. *"Think step-by-step and consider all possibilities…"*). •
- **Output Format Specification:** e.g. *"Provide the answer in JSON with fields X, Y, Z"* or *"Write 3 paragraphs: introduction, analysis, conclusion."* •
- **Examples (Few-Shot):** one or two examples of desired output or solved steps (if applicable to guide style/format). •
- **Final Directive:** a clear call-to-action, e.g. *"Now produce the incident report with the above requirements."* •

This sequence (adapted from best-practice guides) moves logically from context-setting to execution, helping the model **understand what to do and how to do it** . Including **example outputs** is especially powerful – showing the model what a correct solution looks like often clarifies expectations better than description alone . [16](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20is%20foundational%2C%20but%20worth,purpose%20to%20executing%20the%20task) [17](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20structure%20moves%20from%20context,to%20think%20about%20the%20task) [18](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=One%20of%20the%20most%20effective,it%20does%20to%20human%20students)

- **Assign Roles and Perspectives:** Explicitly assign the model a role relevant to the task. Phrasing the prompt as **"You are a X"** can anchor the model's style and knowledge. For instance, *"You are a software engineer assistant,"* or *"Act as a librarian with expertise in history."* Role-based prompts align the model's voice and behavior with the desired context . This helps in tasks like creative writing (e.g. *"You are a novelist narrating in first-person"*) or domain-specific answers (*"You are a legal advisor explaining an issue to a non-lawyer"*). Combining a role with other techniques (examples, chain-ofthought, etc.) often yields the best results . • [19](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=%2A%20Choose%20chain,analyst%2C%20or%20customer%20support%20agent) [20](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Combo%20Example%3A%20Role,thought)
- **Avoid Unnecessary Complexity:** While advanced prompting techniques are useful (and discussed below), always ensure the final prompt is as simple as possible while covering all requirements. Unneeded verbosity or gimmicks (like all-caps or excessive punctuation) does not improve performance . Clarity and logic trump "clever" phrasing. • [21](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Despite%20what%20you%20might%20have,work%20better%20than%20artificial%20emphasis)

By adhering to these principles, an AI agent builds a solid prompt on which complex multi-step strategies can be scaffolded. Next, we explore those advanced strategies – guiding the agent to plan actions, use tools, and verify outcomes within the prompt framework.

## **Planning and Reasoning Strategies for Multi‑Step Tasks**

Complex tasks often require the agent to **plan a sequence of steps** and reason through intermediate results before finalizing an answer. Two key prompt engineering techniques to enable this are **Chain-of-Thought prompting** and its extensions, and **Tree-of-Thought prompting** for branching reasoning. The goal is to prevent the model from "jumping straight to the answer" and instead simulate a logical thinking process or outline.

### **Chain-of-Thought (CoT) Prompting**

*Chain-of-Thought* (CoT) prompting explicitly instructs the model to generate a step-by-step reasoning path, rather than output only the final answer . For example, a prompt might say: *"Explain your reasoning step by step before giving the final answer."* or simply include *"Let's solve this step by step."* as part of the query. This causes the model to produce a sequence of intermediate thoughts ("First, I observe…, Next, I deduce…, Therefore,…") which can greatly improve performance on tasks involving **logic, math, or multi-hop reasoning** . By verbalizing intermediate steps, the model is less likely to skip important details or constraints. [22](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=match%20at%20L694%20Chain,intermediate%20steps%3A%20%E2%80%9CFirst%E2%80%A6%20then%E2%80%A6%20therefore%E2%80%A6%E2%80%9D) [19](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=%2A%20Choose%20chain,analyst%2C%20or%20customer%20support%20agent) [22](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=match%20at%20L694%20Chain,intermediate%20steps%3A%20%E2%80%9CFirst%E2%80%A6%20then%E2%80%A6%20therefore%E2%80%A6%E2%80%9D)

**When to use CoT:** Recent meta-analyses show that CoT provides strong benefits primarily for tasks that are **symbolic or logical in nature** – e.g. mathematics word problems, puzzles, or logical deduction – whereas for simpler factual Q&A, CoT may not add much value . In one study surveying 100+ papers and many benchmarks, CoT significantly improved accuracy on math and algorithmic reasoning tasks, but gave only minor gains on general knowledge queries . In other words, CoT helps when the task is *difficult for the model's "intuitive" mode*, but if the model can answer directly from memory (e.g. a straightforward factual question), the extra "thinking" might be unnecessary. An agent should **apply CoT selectively**, focusing on [23](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=conducted%20a%20quantitative%20meta,CoT%27s%20gain%20comes%20from%20improving) [23](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=conducted%20a%20quantitative%20meta,CoT%27s%20gain%20comes%20from%20improving)

problems that clearly involve multiple reasoning steps or potential pitfalls in reasoning . (It's also worth noting that if a tool or external solver is available for a sub-problem – say a complex calculation – that might outperform the model's chain-of-thought alone . More on tool-use later.) [24](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=CoT%20gives%20strong%20performance%20benefits,can%20be%20applied%20selectively%2C%20maintaining) [25](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=symbolic%20operations%20and%20reasoning,whole%20range%20of%20LLM%20applications)

To implement CoT in a prompt, one can: - Use trigger phrases: e.g. *"Think step by step"*, *"Solve systematically:"*, *"First, … Then, …"*.

- Provide a worked example: *"Example: Q: If Alice has 5 apples and buys 7 more, how many? A: Let's think step by step. Alice starts with 5, then… (answer 12)"*, then follow with the actual question.

- Encourage justification: *"Show your reasoning before giving the answer."*

The model will then output a chain of intermediate conclusions leading to the final answer. This not only improves correctness in many cases , but also provides **interpretability** – the reasoning chain can be reviewed or verified by the agent or user. [26](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Chain,intermediate%20steps%3A%20%E2%80%9CFirst%E2%80%A6%20then%E2%80%A6%20therefore%E2%80%A6%E2%80%9D)

**Self-Consistency:** A refinement of CoT prompting from recent research is the *self-consistency* technique, where the model generates multiple distinct reasoning chains and then the agent selects the most consistent answer among them. While effective in reducing errors, this is computationally expensive and may not be necessary for every task. However, an AI agent could internally prompt itself to "think of alternative solutions" and cross-check results if high reliability is needed.

### **Tree-of-Thought and Exploratory Reasoning**

For very complex problem-solving, **Tree-of-Thought (ToT)** prompting extends the idea of CoT by allowing **branching** at intermediate steps . Instead of linear step-by-step reasoning, the model explores a *tree* of possible approaches or partial solutions, then evaluates which path is most promising. In prompting terms, the agent might instruct: *"Consider multiple possible ways to approach this problem. Start with different assumptions or methods, then evaluate each."* This can be implemented by a prompt like the one proposed by researchers in 2023: *"Imagine three different experts are tackling the question. Each expert will think one step at a time and share their thought. If any expert realizes they're wrong, they drop out. The question is: [X]?"* . This clever prompt causes the model to simulate a panel of solvers, effectively creating a branching discussion (a simple form of ToT prompting). [27](https://www.promptingguide.ai/techniques/tot#:~:text=For%20complex%20tasks%20that%20require,problem%20solving%20with%20language%20models) [28](https://www.promptingguide.ai/techniques/tot#:~:text=Hulbert%20,A%20sample%20ToT%20prompt%20is)

Under the hood, ToT methods may involve the agent guiding the model to use search algorithms (breadthfirst, depth-first search through the solution space) . At each "thought", the model can be asked to evaluate progress – e.g., *"Rate this partial solution as (promising / uncertain / impossible) and justify."* The agent can then prompt the model to continue expanding the promising branches and prune the bad ones . This **deliberate search** mimics human strategic planning (considering multiple options and backtracking as needed). Yao et al. (2023) demonstrated that a systematic ToT approach substantially outperforms basic chain-of-thought on certain complex tasks like solving puzzles (e.g. the 24 game) and reasoning with lookahead . [29](https://www.promptingguide.ai/techniques/tot#:~:text=ToT%20maintains%20a%20tree%20of,thoughts%20with%20lookahead%20and%20backtracking) [30](https://www.promptingguide.ai/techniques/tot#:~:text=ToT%20maintains%20a%20tree%20of,thoughts%20with%20lookahead%20and%20backtracking) [30](https://www.promptingguide.ai/techniques/tot#:~:text=ToT%20maintains%20a%20tree%20of,thoughts%20with%20lookahead%20and%20backtracking) [31](https://www.promptingguide.ai/techniques/tot#:~:text=When%20using%20ToT%2C%20different%20tasks,best%20b%3D5%20candidates%20are%20kept) [32](https://www.promptingguide.ai/techniques/tot#:~:text=From%20the%20results%20reported%20in,outperforms%20the%20other%20prompting%20methods)

For an AI agent, implementing a full tree search might be involved, but even a simplified approach – explicitly asking the model *"List two different ways this problem could be solved, then compare their outcomes."* – can introduce helpful divergent thinking. After exploring alternatives, the agent can prompt the model to *"choose the best solution and explain why."* This ensures some level of internal verification by comparing options.

### **Decomposing Tasks and Stepwise Planning**

Another powerful strategy for multi-step tasks is **task decomposition**: breaking a complex query into smaller, manageable subtasks and addressing them one by one. Rather than asking for the final result directly, the prompt can guide the model through a **sequence of subtasks**. For example, for a question like *"Analyze this product review and draft a balanced summary with pros and cons,"* the agent's prompt might be structured as:

- *"First, summarize the main points the reviewer makes."* 1.
- *"Next, identify at least three pros mentioned."* 2.
- *"Then, identify any cons or criticisms."* 3.
- *"Finally, write a summary that presents the reviewer's opinion, mentioning those pros and cons."* 4.

Each of these could even be separate prompts in a *prompt chaining* approach, where the output of one step is fed into the next . Prompt chaining is indeed a technique where the agent uses multiple successive prompts to handle complex tasks that a single prompt might struggle with . This can improve reliability by focusing the model on one aspect of the task at a time . [33](https://www.promptingguide.ai/papers#:~:text=%2A%20Zero,Retrieval%20Augmented%20Generation) [34](https://www.reddit.com/r/PromptEngineering/comments/1ki9qwb/advances_in_llm_prompting_and_model_capabilities/#:~:text=5%20Advances%20in%20LLM%20Prompting,2025) [35](https://2024.aiwareconf.org/details/aiware-2024-papers/16/Chain-of-Targeted-Verification-Questions-to-Improve-the-Reliability-of-Code-Generated#:~:text=Chain%20of%20Targeted%20Verification%20Questions,various%20nodes%20within%20the)

In scenarios where the agent is allowed to interact in multiple turns, **iterative prompting** or a planexecute-refine loop can be employed. The agent might first prompt the model to output a plan, then inspect that plan (possibly using additional tools or logic), and finally prompt the model again to carry out the plan. An example of this pattern is: *"Outline the steps you will take to solve this."* (Model outputs a plan.) The agent verifies the plan, maybe using code or rules, then says *"Great. Now execute those steps one by one and show the results."* If an intermediate result is unsatisfactory, the agent can intervene with another prompt like *"Adjust step 3 to account for X and then continue."*

**Query Decomposition & Synthesis:** The medium-term state-of-the-art suggests a prompt structure for analytical tasks: **(1) Query Decomposition** – *"Break down the problem and list what needs to be done or answered in parts,"* **(2) Information Gathering/Analysis** – *"For each part, analyze relevant information or use provided data/tools,"* and **(3) Synthesis** – *"Integrate the findings to produce the final answer."* . By explicitly walking the model through these phases in the prompt, the agent ensures that no aspect of a complex query is overlooked. This is analogous to how a human might first outline an essay, then fill in each section, and finally write a conclusion tying everything together. [36](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Query%20Decomposition%3A%20,together%20into%20a%20cohesive%20analysis) [37](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Synthesis%20and%20Integration%3A%20,together%20into%20a%20cohesive%20analysis)

**Example – Analytical Task Prompt:** Suppose the task is to evaluate a company's financial health from a report. A prompt leveraging decomposition might be:

*"You are a financial analyst. Step 1: Break down the analysis: list the key areas (e.g. profitability, liquidity, etc.) you will assess. Step 2: For each area, use the data provided in the report below to evaluate the company's status (cite numbers). Step 3: Finally, synthesize these insights into an overall conclusion about the company's financial health. Now begin with Step 1."*

This instructs the model to first produce an outline of sub-problems, then handle each with evidence, and then conclude – a clear plan embedded in the prompt. It closely follows the *"plan before action"* principle that improves reasoning quality . [38](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=to%20how%20we%20might%20teach,essay%20question%20into%20smaller%20components) [36](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Query%20Decomposition%3A%20,together%20into%20a%20cohesive%20analysis)

Chain-of-thought, tree-of-thought, and decomposition techniques all serve the larger goal of **prompting the model to plan its actions and reasoning** before final execution. By 2025, it is evident that *planning prompts* significantly enhance performance on tasks that are too complex to solve in one shot . An AI agent should internalize these patterns, using them to tackle problems systematically rather than reflexively. However, planning alone is not enough – the agent must often **interface with external tools** or sources of information to execute the plan and then verify the results. We turn to those aspects next. [39](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=For%20complex%20problems%20requiring%20careful,essay%20question%20into%20smaller%20components) [40](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20structured%20reasoning%20approach%20helps,clear%20sequence%20for%20addressing%20them)

## **Tool Use and Agentic Prompting**

Modern AI agents like Claude with code execution abilities or GPT-4 with plugins can perform actions in the world: searching the web, running code, querying databases, manipulating files, etc. **Agentic prompting** refers to designing prompts that not only make the model *think*, but also *act* – instructing it to use tools and incorporate the results of those actions into its reasoning. This is crucial for complex tasks that require information beyond the model's training data or that involve real-world operations (e.g. executing a bash command to list files, or retrieving current data from the web).

One influential framework is **ReAct: Reason + Act** . The ReAct paradigm prompts the model to intermix *reasoning traces* (natural language thoughts) with *action commands* that interface with tools . A ReActstyle prompt might look like this (illustrative example): [41](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=In%20%E2%80%9CReAct%3A%20Synergizing%20Reasoning%20and,ReAct%29%20paradigm%20systematically) [42](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L264%20ReAct%20enables,affect%20the%20internal%20state%20of)

You are an autonomous AI agent. You have access to the following tools:

- SearchTool[query]: searches the web for the query.
- CalcTool(expression): evaluates a mathematical expression.
- BashTool(command): executes a bash command in the system.

```
Instruction: Solve the user's request step by step. 
For each step, you may (1) **think** (in natural language), (2) optionally 
**act** by calling a tool, and (3) observe the tool's output.
Finally, provide the answer to the user.
```

User request: "Find the latest stock price of XYZ Corp and save it to a file."

Now begin.

In the above scheme, the model when prompted will generate a sequence like:

Thought: "I need to find XYZ Corp stock price." •

- Action: SearchTool["XYZ Corp stock price"] *(the environment returns some results)*. •
- Observation: *"XYZ Corp (NYSE: XYZ) is trading at \$123.45 as of Aug 25, 2025."* (from search result) •
- Thought: "Got the price. Next, I should save it to a file." •
- Action: BashTool["echo 'XYZ: \$123.45' > stock.txt"] *(executing the command)* •
- Observation: *"(no output)"*. •
- Thought: "The data is saved. Task complete." •
- Final Answer: *"The latest price of XYZ Corp is \$123.45, which I have saved to* stock.txt\*\*\*." •

This is a simplified illustration, but it shows how the model is guided to **alternate between reasoning and acting**, using observations from the environment to inform next steps . The ReAct paper demonstrated that this approach enables LLMs to handle interactive tasks (question answering with retrieval, fact checking, game environments, etc.) much more effectively than either reasoning alone or acting without reasoning . In fact, combining chain-of-thought reasoning with tool use (ReAct+CoT) gave the best results, as the model could leverage both its internal knowledge and external information sources . [43](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=ReAct%20enables%20language%20models%20to,affect%20the%20internal%20state%20of) [44](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=Wikipedia%20API%20with%20which%20the,both%20internal%20knowledge%20and%20externally) [45](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L324%20Wikipedia%20API,both%20internal%20knowledge%20and%20externally) [45](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L324%20Wikipedia%20API,both%20internal%20knowledge%20and%20externally)

**Designing Prompts for Tool Use:** To instruct an AI agent to use tools, the prompt should clearly enumerate what tools are available and how to invoke them (often a special syntax or style, as in the example above). The agent should be reminded of any usage constraints (e.g. not to reveal the tool outputs directly if that's undesired, or to sanitize results). Crucially, the prompt must encourage the agent to **not hallucinate information** but instead use tools whenever it is unsure. A directive from recent practice says: *"When facing uncertainty, utilize available tools to gather data—never resort to estimation."* . This kind of instruction can be embedded in the system message or prompt. By following it, the model reduces hallucinations, since it knows to fact-check itself via a tool rather than guessing . [46](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem) [46](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem)

For example, if a user asks a complex factual question, the prompt to the agent might be: *"If you are not fully confident about a factual detail, use the SearchTool to verify it before answering. Do not fabricate quotes or stats."* This aligns with the **Information Verification** concept introduced earlier . It turns the AI into a more **truthful agent**, one that consults sources when needed. [47](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=model%20from%20stopping%20halfway%20through,to%20plan%20an%20essay%20before) [46](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem)

**Persistence and Autonomy:** In a multi-step operation, an agentic prompt should also handle the flow of control. The agent should continue working through steps without needing repeated user queries. A "persistence protocol" can be prompted: e.g. *"Continue executing steps until the goal is achieved, instead of stopping to ask the user for next step."* . By telling the model it won't get further input until completion, we prevent it from pausing prematurely. This is especially relevant when the agent has to make multiple tool calls or iterate until a condition is met. [48](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,Implement%20planning%20before%20each)

**Methodical Action and Outcome Analysis:** The agent should be prompted to not only perform actions but also check the outcomes of those actions. A good practice is: *"Implement planning before each action, followed by analysis of the outcome."* . In other words, after each tool use, the model should examine the result: Did the action succeed? Does the result suggest a new approach? For instance, if a BashTool command returns an error, the agent's next thought (guided by the prompt) should be to diagnose that error and adjust the plan – rather than blindly continuing. By building this **feedback loop** into the prompt, we emulate how a careful human problem-solver operates: plan, act, observe, adjust. This ties closely into verification techniques discussed in the next section. [49](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=reduces%20hallucination%20by%20encouraging%20the,after%20completing%20a%20math%20problem)

In summary, **agentic prompt engineering** empowers the AI to act in an environment. The latest research underscores that LLM agents perform best when prompts encourage a tight integration of reasoning with action . The agent should be explicitly instructed to use tools for tasks like web search, calculations, or file operations instead of trying to do everything with pure text prediction. It should also be taught to maintain an internal state or memory of what it has done (e.g. which files have been created, which web results were found) so it can refer back to those in subsequent reasoning. This can be achieved by prompting it to **summarize or note important results** after each action, effectively building an "episodic memory" of the session. [50](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=ReAct%20demonstrates%20the%20feasibility%20of,of%20research%20and%20leverage%20the)

We will now look at how the agent can verify and refine its outputs – the crucial last stage to ensure accuracy and completeness.

## **Verification and Self-Correction Mechanisms**

Even with good planning and tool use, the first attempt at a complex task may not be perfect. The AI agent must be equipped with **verification and self-correction strategies** via prompting. This involves checking its own intermediate or final outputs for errors, inconsistencies, or unmet requirements, and then refining as needed. Two notable approaches from recent research are the **Chain-of-Verification (CoVe)** method for factual accuracy and the **Reflexion** framework for iterative self-improvement. Additionally, prompt-driven unit testing and result validation are vital in domains like coding or file operations.

### **Chain-of-Verification (CoVe) for Factual Accuracy**

When answering knowledge-intensive questions or generating detailed content, LLMs can produce **hallucinations** – plausible-sounding but incorrect statements. The Chain-of-Verification approach, introduced in 2023–2024, addresses this by having the model *verify each part of its answer in a separate reasoning chain* . In practice, a CoVe prompt might work as follows: [6](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20Chain,reducing%20the%20occurrence%20of%20hallucinations)

- **Initial Answer Generation:** The model produces an initial answer to the user's query (possibly with CoT reasoning). 1.
- **Question Generation (Verification Plan):** The model then is prompted to generate a set of **verification questions** about its answer – essentially, probe questions that, if answered, would confirm or refute parts of the answer . For example, if the answer stated a specific fact, the model would pose a question like *"Is it true that ... [fact] ... ?"*. 2. [51](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=information%E2%80%94%20in%20Large%20Language%20Models,reducing%20the%20occurrence%20of%20hallucinations)
- **Independent Fact-Checking:** For each verification question, the model (or agent via a tool like a search engine) finds the answer *without seeing the original answer* (to avoid confirmation bias) . This could mean retrieving source information or using its stored knowledge. 3. [51](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=information%E2%80%94%20in%20Large%20Language%20Models,reducing%20the%20occurrence%20of%20hallucinations)
- **Result Comparison and Revision:** The model then compares the verification findings with its original answer and identifies discrepancies or unsupported claims. Finally, it is prompted to *"Revise the answer to correct any inaccuracies, using the verified information."* 4.

By explicitly going through this verify-and-edit loop, the occurrence of factual errors can be greatly reduced . Dhuliawala et al. (2024) showed that CoVe significantly decreased hallucinations in tasks like QA and long-form generation . The process forces the model to **scrutinize its own output** before finalizing it. An AI agent can incorporate this by, for example, appending to its prompt: *"Double-check each claim in your solution. For each claim, ask yourself a question: how can I verify this? Answer those questions using known facts or tool results, then correct the solution if any claim is wrong or unsubstantiated."* Essentially, the agent becomes its own fact-checker. [52](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [52](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs)

In a practical agentic setting, the verification questions could be answered by calling search or a knowledge base. For instance, after the model generates a draft answer about a historical event, it might form a verification question like *"What does Encyclopedia X say about the event date?"*. The agent would then use a search tool to get the answer and compare. The final prompt to produce the "verified answer" would include the instruction to incorporate only confirmed facts. This approach has proven effective at improving **reliability and trustworthiness** of LLM outputs . [53](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [54](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20development%20and%20implementation%20of,more%20reliable%20and%20accurate%20LLMs)

### **Reflexion and Self-Improvement**

Verification can also occur at the level of overall task success, not just factual accuracy. A recent paradigm called **Reflexion** (Noah Shinn et al., 2023) gives agents a way to learn from their mistakes within prompting sessions . Unlike CoVe, which is focused on checking facts in a single answer, Reflexion is about *strategic self-assessment* and improvement over multiple attempts. [55](https://www.promptingguide.ai/techniques/reflexion#:~:text=Reflexion%20is%20a%20framework%20to,a%20choice%20of%20LLM%20parameters) [56](https://www.promptingguide.ai/techniques/reflexion#:~:text=In%20summary%2C%20the%20key%20steps,reflection%20and%20memory%20components)

In Reflexion, after an agent completes a task (or fails to), it generates a **self-reflection**: a concise analysis of what went wrong or right and what could be improved . This reflection is then stored in the agent's memory and injected into the next trial's prompt as feedback. For example, if the agent tried to solve a puzzle and failed, the system might prompt: *"Analyze your previous attempt and explain why it failed. What hint or correction can you derive?"* The model's answer (the reflection) might be: *"I realize I assumed X, which was incorrect. I should instead try Y in the next attempt."* This reflection is then prepended to a new prompt for the task (or kept in a persistent memory slot) so that the model does not repeat the same mistake . Essentially, the agent is using verbal feedback as a form of reinforcement learning, without updating weights – it updates the prompt context instead. [57](https://www.promptingguide.ai/techniques/reflexion#:~:text=At%20a%20high%20level%2C%20Reflexion,improvements%20on%20many%20advanced%20tasks) [58](https://www.promptingguide.ai/techniques/reflexion#:~:text=%2A%20Self,making) [59](https://www.promptingguide.ai/techniques/reflexion#:~:text=%2A%20Self,making) [56](https://www.promptingguide.ai/techniques/reflexion#:~:text=In%20summary%2C%20the%20key%20steps,reflection%20and%20memory%20components)

Reflexion frameworks often couple with approaches like ReAct and CoT. For instance, an agent might use ReAct to interact with an environment, fail or succeed, then Reflexion kicks in to modify the next ReAct sequence . Studies showed that Reflexion-enabled agents achieved higher success rates on decisionmaking tasks (like navigating text-based environments) and even improved code generation success on programming puzzles . In one case, a Reflexion agent was able to boost its score on a Python coding benchmark significantly by learning from errors and generating new attempts with those lessons in mind . [60](https://www.promptingguide.ai/techniques/reflexion#:~:text=ReAct%20,relevant%20feedback%2C%20which%20is%20also) [61](https://www.promptingguide.ai/techniques/reflexion#:~:text=can%20learn%20to%20iteratively%20optimize,reflection%20and%20memory%20components) [62](https://www.promptingguide.ai/techniques/reflexion#:~:text=Experimental%20results%20demonstrate%20that%20Reflexion,Python%20programming%20tasks%20on%20HumanEval) [63](https://www.promptingguide.ai/techniques/reflexion#:~:text=Reflexion%20significantly%20outperforms%20all%20baseline,CoT%20with%20episodic%20memory%2C%20respectively) [64](https://nips.cc/virtual/2023/poster/70114#:~:text=Reflexion%3A%20language%20agents%20with%20verbal,buffer%20to%20induce%20better) [65](https://www.promptingguide.ai/techniques/reflexion#:~:text=As%20summarized%20in%20the%20table,MBPP%2C%20HumanEval%2C%20and%20Leetcode%20Hard)

For our AI agent aiming to be a master prompt engineer, applying Reflexion means always asking after an attempt: *"Did I fulfill all parts of the instruction correctly? If not, which part is missing or incorrect?"*. The agent can be prompted to do this self-critique automatically. A possible prompt pattern: *"Now reflect: Compare your output against the requirements. If something is wrong or missed, explain the issue and how to fix it."* Then, *"Using that reflection, attempt the task again, fixing the issues."* This essentially implements a one-shot Reflexion loop. If the platform allows, the agent can loop this until a verification check passes (or a limit is reached).

One must be cautious with infinite loops, but typically 1-2 iterations of self-refinement can dramatically improve quality on complex outputs . The *meta-prompting* technique from earlier is also relevant here: instructing the model to evaluate its prompt or output against explicit criteria . For example, after generating a long document, the agent could be prompted: *"Evaluate the above output for clarity, completeness, and correctness. List any issues found."* Then: *"Now revise the output to address those issues."* By giving the model a "critic" role and then an "editor" role, we encourage a higher standard of output. [53](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [66](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Original%20Prompt%3A%20)

### **Testing and Verification in Code or Action Domains**

Verification is perhaps most obviously needed in **software engineering tasks**, where outputs (code) can be objectively tested. Prompt engineering for coding should include instructions to self-check the code for errors. State-of-the-art methods in 2024 introduced *"chain-of-targeted-verification questions"* specifically to improve code reliability . In this approach, after initial code generation, the agent forms targeted questions about potential bugs in that code, focusing on likely problematic areas (e.g. edge cases, error [67](https://arxiv.org/abs/2405.13932#:~:text=required%20by%20the%20user%20to,targeted%20VQs%20and%20the%20initial) [68](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,tasks%20in%20the%20CoderEval%20dataset)

handling). For instance, if the code includes a sorting algorithm, a verification question might be *"What happens if the input array is empty or has one element?"*. The agent then answers those questions (essentially simulating or reasoning through unit tests) and uses the insights to catch bugs . Research has shown this method can reduce bugs significantly before any code execution . [69](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,targeted%20VQs%20and%20the%20initial) [70](https://arxiv.org/abs/2405.13932#:~:text=and%20in%20the%20absence%20of,tasks%20in%20the%20CoderEval%20dataset) [71](https://arxiv.org/abs/2405.13932#:~:text=target%20various%20nodes%20within%20the,executable%20code%20instances%20to%2013)

An AI agent acting as a coder can integrate this by prompting itself: *"Think of at least 3 potential edge cases or failure scenarios for the above code."* Then: *"Check mentally or with tests how the code handles those. If any issue is found, fix the code."* This is analogous to performing a code review or dry-run tests via prompting.

Better yet, if the agent can execute code (which our Claude-code scenario allows), it should be prompted to actually run tests. For example, the prompt might instruct: *"After writing the function, generate a few unit tests and execute them. If any test fails, debug and correct the code."* This is basically an automated Test-Driven Development loop. Indeed, developers have started to automate TDD with LLMs: having the model create thorough unit tests from the specification, then writing code to pass those tests . A simplified prompt example for generating tests: [72](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=unit_test_system_prompt_string%20%3D%20,of%20the%20function%27s%20intended%20behavior) [73](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=2,Special%20characters)

*"You are a testing assistant. Produce a set of unit tests (using pytest) for the following function description, covering typical cases, edge cases, invalid input, etc. Do not implement the function, only tests."* . [72](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=unit_test_system_prompt_string%20%3D%20,of%20the%20function%27s%20intended%20behavior) [74](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=expected%20behavior%2C%20including%3A%20,Special%20characters)

After generating tests, the agent can run them (via a tool) to see which fail, then instruct the model to fix the code accordingly. This multi-step prompt-tool chain greatly improves code quality without human intervention, as shown by Kouemo Ngassom et al. (2024) who achieved up to 21–62% reduction in certain code errors using targeted re-prompting for bug fixes . [71](https://arxiv.org/abs/2405.13932#:~:text=target%20various%20nodes%20within%20the,executable%20code%20instances%20to%2013)

Beyond code, any *action sequence* should be verified. For example, for **file management tasks** (where the agent uses bash commands to manipulate files), the prompt should include a post-action check. If the user asks to, say, "organize files by date," the agent's plan (prompted internally) might be: (1) list files, (2) create directories for each date, (3) move files, (4) verify each file moved to correct folder, (5) confirm completion. The agent should not consider the task done until it has executed step (4) and confirmed the expected outcome. We can enforce this with a verification step in the prompt: *"After performing the actions, list the resulting directory structure to ensure it matches the intended outcome."* If the output deviates, the agent should detect it (perhaps by a simple comparison or a rule in the prompt: *"If any file is not in the expected location, retry the move or report an error."*).

Interestingly, formal verification from robotics is making its way into LLM agent planning. Some researchers translate a high-level plan into logical constraints and have the LLM reason about those to catch impossible or illogical steps . For example, *"Verify step 3 follows from step 2 and does not violate any prerequisites"* could be a prompt instruction. In one system (VerifyLLM), the model would detect errors like a robot trying to pour water into an upside-down glass by reasoning about physical logic, then suggest reordering steps . For file tasks, the equivalent might be ensuring a file is opened only after it exists, etc. As an AI agent, you should incorporate common-sense and domain rules into your verification prompts: *"Check that the action sequence respects all prerequisites (e.g., don't read a file before it's created). If a prerequisite is missing, insert that step."* . This leads to more **robust and error-free execution**. [75](https://arxiv.org/html/2507.05118v1#:~:text=consists%20of%20two%20key%20steps%3A,io) [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites) [77](https://arxiv.org/html/2507.05118v1#:~:text=Verifying%20robot%20action%20plans%20before,humans%20naturally%20take%20into%20account) [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites) [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites)

To summarize, verification in prompt engineering involves multiple levels: **factual verification** (CoVe) for content accuracy, **output compliance verification** (checking all instructions were followed and output format is correct), and **action outcome verification** (ensuring that each step achieved the intended result in the environment). By building these checks and balances into prompts, an AI agent minimizes errors and can approach tasks with a high degree of autonomy and reliability. In the next section, we consolidate these techniques in specific domains, providing concrete examples of prompts and strategies tailored to different complex tasks.

## **Domain-Specific Prompt Strategies**

Different problem domains benefit from specialized prompt tactics. Here we outline how an AI agent can optimize prompts for: 1) **Software Engineering (Code Generation & Analysis)**, 2) **Document Writing & Summarization**, 3) **Creative Writing**, 4) **Analytical Problem Solving**, and 5) **File Management / Tool Automation**. Each subsection highlights unique challenges and best practices for that domain, building on the general techniques discussed above.

### **1. Software Engineering and Code Generation**

**Challenges:** Writing code with LLMs involves understanding requirements, producing syntactically correct and efficient code, and avoiding bugs or vulnerabilities. There's also often a need to generate not just code, but also explanations or documentation. Ensuring the code actually works (and meets the spec) is the ultimate measure of success, so testing and debugging are integral.

### **Prompt Strategies:**

- **Specify the development role and objective:** Begin prompts with a role like *"You are an expert Python developer"* and clearly state what needs to be built or fixed. Include any constraints (e.g. runtime complexity, libraries to use/avoid, coding style guidelines). For example: *"Write a Python function to merge two sorted lists into one sorted list. Do not use built-in sort functions. The code should be efficient (O(n) time)."* This sets expectations upfront. •
- **Decompose the problem (if complex):** For a complicated coding task, ask the model to outline a solution approach first. *"Step 1: Plan the solution in pseudocode or steps,"* then *"Step 2: Write the code following that plan."* This can prevent the model from diving into coding with a flawed approach. It's similar to prompting CoT: *"Let's think about how to solve this before coding."* •
- **Use In-Code Comments as Guidance:** You can prompt the model to produce commented code, where the comments act as an internal reasoning trace. For example: *"Include comments explaining each part of the code."* This not only makes the output more understandable, but if the model starts with comments (outline) and then fills code, it's effectively following a plan it wrote in natural language. •
- **Emphasize verification with tests:** As discussed, instruct the model to generate tests or consider edge cases. e.g. *"Before finalizing, list some edge cases and how your code handles them."* Or *"Provide 2-3 example inputs and the expected outputs for the function as a quick test."* . This forces the model to simulate running the code mentally. If your environment allows, you can literally have the agent run these examples using its tool capabilities, but even without execution, the prompt can encourage a mental check. • [69](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,targeted%20VQs%20and%20the%20initial)

- **Error handling and safety:** Prompt the model to handle potential errors (invalid inputs, exceptions) rather than assuming perfect input. *"If input may be malformed, include error checks and handle gracefully."* This improves robustness of generated code. •
- **Iterative debug loop:** If the model's first code attempt might be wrong, consider a loop in the prompt: *"If the code is not correct or fails a test, analyze the failure and fix the code."* The agent can catch runtime or logical errors by actually running the code (if possible) or by reasoning (if obvious, e.g. syntax errors can be caught by the model's internal knowledge of code grammar). There are prompting techniques specifically for self-debugging code: some approaches have the model generate a hypothesis of what could be wrong and then a corrected version . For instance, *"Analyze the above code for bugs and inefficiencies. If any are found, output a revised version of the code fixing them."* • [78](https://openreview.net/forum?id=vAElhFcKW6#:~:text=learning%20openreview,for%20coding%20Reflexion%20has)

#### **Example Prompt (Code Generation with Self-Check):**

```
You are a C++ programming assistant. Write a function `int 
findMedian(vector<int>& nums)` that returns the median of a list of integers.
Requirements:
- If `nums` has an even number of elements, return the lower of the two medians.
- Do not use library sorting functions.
- Ensure the solution is efficient in time and space.
First, outline your approach in comments (steps to solve). 
Then write the function code. 
After coding, provide 2-3 example tests with expected outputs to demonstrate 
correctness.
```

In this prompt, the instructions guide the model to first plan (in comments), then implement, then verify with examples. An agent following this prompt would produce an answer like: comments describing using a selection algorithm or partial sort, the code, and example usage tests. The examples act as a quick verification. If the platform allowed, the agent could even execute those tests to double-check.

**Reference to Latest Research:** Techniques like generating **Verification Questions** for code (asking about each part of the code's logic) have been shown to reduce bugs significantly . Encouraging the model to think in terms of an Abstract Syntax Tree or logic flow can catch errors. Also, the Reflexion approach applied to code means if an initial attempt fails (e.g. doesn't compile or doesn't pass a given example), the model should reflect: *"It failed for input X, likely because of Y bug,"* then try again . As a prompt engineer, ensure the agent always has that next step: not stopping at a wrong answer but attempting to fix it (within reasonable iteration limits to avoid infinite loops). [69](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,targeted%20VQs%20and%20the%20initial) [79](https://openreview.net/forum?id=vAElhFcKW6#:~:text=Debugging%20,for%20coding%20Reflexion%20has)

### **2. Document Writing and Technical Documentation**

**Challenges:** Writing a document (e.g. an article, report, or user manual) is a complex task that involves organizing content, maintaining coherence over many paragraphs, and meeting specific formatting or content requirements. The agent must manage long-form generation without losing track of the outline or repeating itself.

#### **Prompt Strategies:**

- **Define structure in advance:** A key to long documents is to have a clear outline. The prompt can explicitly instruct the model to produce or follow a certain structure. For example: *"Write a research report with the following sections: Introduction, Methods, Results, Conclusion. Begin by giving an outline of what each section will contain, then fill in each section in detail."* If the structure is known (like a standard template or the user provides headings), ensure the model sees those headings and fills under each. •
- **Role and style:** Set the tone via role assignment. *"You are an AI writing assistant that produces clear, formal technical documentation."* Also specify style guidelines: *"Use a formal tone, avoid first person, and include bullet lists for key points."* This ensures consistency throughout the document. •
- **Chunking the task:** If the document is very long or complex, it might be wise to split it into parts. The agent could generate section by section in a chain, either automatically or via intermediate prompts. When doing this autonomously, the agent should keep track of previous sections (perhaps summarizing them or storing them in memory) to maintain coherence. The prompt for each section could include a brief summary of what's already written: *"You have written the Introduction and Methods. Now write the Results section. Remember that the Introduction stated the hypothesis was XYZ."* This is a way to keep context without exceeding token limits. •
- **Content completeness checklist:** Provide in the prompt a checklist of points to cover. For example: *"In the Conclusion, ensure you address: (a) whether the hypothesis was supported, (b) any limitations, (c) suggestions for future work."* The model will then be prompted to cover each item. You can format this as bullet points in the prompt to make it clear. •
- **Use of Tools for data/information:** If the document requires data (e.g. include the latest statistics, or incorporate quotes), instruct the agent to use tools to fetch that information before writing. For instance, *"If any factual data is needed (like year or figures), use the search tool to find the latest info, and cite it."* The agent can pause writing, do the search, then resume writing with the found data, thereby increasing accuracy. •
- **Quality control via self-review:** After drafting the whole document, prompt a self-review. E.g. *"Now review the entire document for clarity, coherence, and any missing information. List any changes or additions needed."* Then, *"Incorporate those changes into the final draft."* This is similar to the metaprompting approach where the model critiques its output . • [66](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Original%20Prompt%3A%20)

#### **Example Prompt (Document Drafting):**

```
You are a technical writer. Produce a 2-page whitepaper on "The Benefits of 
Prompt Engineering in 2025". 
Structure:
- **Title** (a concise, informative title)
- **Introduction** (explain what prompt engineering is and why it matters in 
2025)
```

```
- **Key Techniques** (at least 3 subsections: e.g. Few-Shot Learning, Chain-of-
Thought, Tool Use, etc., each explaining the technique)
- **Case Study** (one short example of prompt engineering solving a real 
problem)
- **Conclusion** (summarize benefits and future outlook)
Requirements:
- Use an academic but accessible tone.
- Include one short illustrative example prompt in a code block.
- Use bullet points or numbering to list techniques in the Key Techniques 
section.
- Ensure no section exceeds 3 paragraphs for readability.
Begin by writing the Title and Introduction.
```

The agent would first output a Title and Introduction. A sophisticated agent might then continue with the other sections, or depending on design, the agent might wait after Introduction if the system expects a check. But since our agent is autonomous, it can see that the prompt lists all sections and proceed to generate the full document. The structure and requirements are clearly laid out, guiding the model's content and style.

**Tips:** The prompt above demonstrates how giving **specific section names** and even desired lengths (like "2 page" as a rough guide, or instructing "no section too long") helps maintain focus. Current models are quite capable of following such structured prompts . The agent should also pay attention to formatting cues – e.g. using Markdown headings for each section as shown, which improves readability as required by the user. We provided an example of bullet points within the content requirement, which the model will likely mimic in output. [80](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=1,a%20clear%20call%20to%20action)

If the document needs figures or references, one can prompt for placeholders: *"If relevant, mention [Figure X] as needed; actual figures will be added later."* or *"Cite sources in APA format when making claims."* The agent can comply accordingly.

Finally, remain aware of **context length**. If a document is extremely long, the agent might need to summarize parts to itself to avoid forgetting earlier content due to context window limits. As a prompt engineer, you might need to instruct the agent to do that summarization of previous parts before moving on, as mentioned.

### **3. Creative Writing and Storytelling**

**Challenges:** Creative tasks (storytelling, poetry, dialogue) require the model to exhibit originality, maintain narrative consistency (characters, plot, tone), and sometimes adhere to specific styles or genre conventions. Unlike technical tasks, there may not be a single "correct" output, but rather an engaging or thematically appropriate one. The agent must plan the narrative arc and possibly incorporate feedback to refine the story.

#### **Prompt Strategies:**

- **Establish context and style via roles:** Set the scene for creativity. For example: *"You are a Shakespearean storyteller"* or *"Act as a dungeon master narrating an epic fantasy tale."* This immediately gives the model a persona and style to emulate. You can mention influences or examples: *"Write in the style of a noir detective novel."* The model will pick up on genre cues and adjust language (e.g. more metaphorical, archaic, etc., depending on prompt). •
- **Outline the plot or structure if known:** If the user or scenario provides some story beats (e.g. characters, setting, conflict), list them. *"Story requirements: Protagonist is a young wizard; must include a dragon; story has a happy ending."* Also, if a certain structure is desired (like a three-act structure or a beginning, middle, end), prompt the model accordingly: *"First, outline the story: beginning situation, the conflict in the middle, how it resolves in the end. Then write the story based on that outline."* This ensures the story stays coherent and hits key points. The model can produce an outline like "Act I: ... Act II: ... Act III: ..." and then flesh it out. The agent should then remove the outline (unless it's supposed to be in the output) or incorporate it seamlessly. •
- **Use creative constraints as prompts:** Sometimes adding constraints can spark creativity and ensure the output meets them. For example: *"Include a flashback scene in the story"*, or *"Use vivid imagery related to the four seasons throughout the narrative."* The model will treat these as challenges to weave into the story. This technique also keeps the story aligned with user expectations if they have specific elements in mind. •
- **Iterative refinement for style and content:** After an initial story draft, the agent can be prompted to refine. This is akin to an editor pass. *"Review the story for narrative coherence and character consistency. If a character's actions seem implausible, adjust the story to fix this. Also, enhance the descriptive language in key scenes."* This allows the model to polish the story. If the story is long, the agent might even split this: first check coherence (does the plot make sense?), then a second pass to beautify language. Because creativity can always be improved, the agent should know it's acceptable to rewrite portions. Just as a human writer might do multiple drafts, the AI can do multi-pass generation when instructed. •
- **Dialogue and character voice:** If writing dialogues or multiple characters, the prompt can list characters and their traits so that the model keeps the voices distinct. E.g. *"Characters: Alice (curious, young), Bob (wise, old). When writing dialogue, keep Alice's tone light and questioning, Bob's tone measured and insightful."* This way, the model maintains consistency in how each character speaks. •
- **Stay on theme:** It's easy for an AI to ramble off-topic in a long creative piece. Remind the model of the theme or moral if needed: *"Ensure the theme of redemption is central throughout the story."* Reiterating this in the prompt can help anchor the narrative. •

#### **Example Prompt (Story Creation):**

You are a storyteller AI known for imaginative fairy tales.

Write an original fairy tale (500-700 words) titled "The Dragon of Evernight".

\*\*Requirements/Guidelines:\*\*

- Setting: A medieval kingdom facing a mysterious endless night.
- Main characters:
- Sir Galen, a knight afraid of the dark (protagonist).
- Evernight Dragon, a misunderstood creature causing the endless night.

- Story should have a beginning (introduction of the problem), a middle (journey and confrontation), and an end (resolution).

- Include at least two dialogues between Sir Galen and the Dragon.
- Maintain a magical, hopeful tone (suitable for children).
- End with a clear moral or lesson.

Start by briefly outlining the plot (in one paragraph), then tell the story in narrative form.

In this prompt, the agent is given a clear creative brief: title, key elements (setting, characters, required structure, tone, and even the presence of dialogue and a moral). We first ask for a quick outline to enforce a structure, then the narrative. The agent will likely produce an outline summarizing the three parts, then delve into a story. This ensures we get a coherent tale that follows classic fairy tale structure and meets the specific elements (fear of dark, endless night, conversation with dragon, etc.).

During generation, if the model forgets to include the dialogues or moral, our prompt explicitly listed them as requirements, so it's more likely to comply. If it still misses one, we could prompt a verification: *"Does the story include dialogues and a moral? If not, add them appropriately."* The agent can then insert what's missing. This shows how even in creative tasks, a bit of verification prompt can enforce the inclusion of key elements.

**Latest Insight:** Creative prompting is less studied in academic literature compared to reasoning tasks, but practitioners find techniques like **persona assignment** and **story outlines** very effective (role-based prompting is a generic form of persona assignment that works for creative styles too). Also, multi-agent prompting (like having the model simulate a group of characters or even having two instances of the model "chat" as characters) can yield rich dialogues; an advanced agent might set up an internal prompt like *"Alice: [says something]\nBob: [responds]..."* to generate conversations. This is a kind of inner prompting trick the agent can use if needed to fulfill the dialogue requirement. 81

Overall, the key is to give enough **guidance to direct the creativity** without micromanaging every word. The agent should feel "free" to invent within the bounds given – which a well-crafted prompt achieves.

### **4. Complex Problem Solving (Logic, Math, etc.)**

**Challenges:** For math problems, logical puzzles, or multi-fact question answering (like solving a murder mystery or troubleshooting a system), precision and correct reasoning are paramount. These often benefit from step-by-step deduction and occasionally require external knowledge or calculation.

#### **Prompt Strategies:**

- **Explicit chain-of-thought:** As covered earlier, always prompt the model to show its reasoning. For instance: *"Solve the following problem step by step. Explain each step of your reasoning and then give the answer."* This not only helps the model get it right but also allows the agent or user to follow the logic. If the answer is wrong, the chain-of-thought can highlight where things went awry. •
- **Break the question into sub-questions:** If a problem is compound (e.g. "Given these clues, who committed the crime and what was the motive?"), the agent should split it: perhaps first determine possible suspects, then evaluate motives, etc. The prompt can encourage this: *"Break the problem into smaller questions: 1) identify ..., 2) determine ..., then combine the results."* The medium decomposition approach we discussed is very useful for logic puzzles . • [36](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Query%20Decomposition%3A%20,together%20into%20a%20cohesive%20analysis)
- **Use formalism if helpful:** Sometimes translating a problem into a formal representation (equations, logic formula, code) can help solve it. The agent can be instructed: *"If appropriate, translate the problem into equations or formal logic before solving."* For example, a puzzle about truth-tellers and liars might be easier if the model sets variables for each person being truth-teller or liar. Similarly, a complicated arithmetic can be done more reliably if laid out as equations. The model might do this on its own, but prompting it explicitly to do so ("Write down the equations needed to solve this") can ensure it follows a robust approach. •
- **Invoke tools for calculation:** For arithmetic or data lookup, it's wise not to rely solely on the model's internal ability (which can sometimes make mistakes in arithmetic). The prompt can say: *"If any complex calculation is required, use the CalcTool."* The agent can thus offload heavy math to a calculator tool. Or for logic, maybe a specialized solver if available. The agent should decide when to trust the model's own reasoning vs. when to use an external function. •
- **Validate the final answer:** For math, a great habit is to have the model double-check the answer by plugging it back into the problem. Prompt: *"After finding the answer, verify it satisfies all conditions of the problem."* The model can then for example substitute the solution into equations or re-check the puzzle constraints. If something doesn't match, the prompt should allow the model to correct itself: *"If the verification fails, revise your solution."* This is essentially the CoVe principle applied to math/ logic: verify each part. •
- **Few-shot examples of reasoning:** If the model struggles, providing an example of a solved similar problem (with reasoning shown) in the prompt can guide it. Few-shot prompting remains a powerful tool in 2025 for getting the model into the right problem-solving mode . The example acts as both instruction and inspiration for the model's chain-of-thought. • [33](https://www.promptingguide.ai/papers#:~:text=%2A%20Zero,Retrieval%20Augmented%20Generation)

#### **Example Prompt (Logical Puzzle):**

You are a logical reasoner. Solve the puzzle below, showing each step of your reasoning:

\*\*Puzzle:\*\* Alice, Bob, and Charlie are three people. One always tells the truth, one always lies, and one alternates between truth and lie. They each made

```
a statement:
- Alice: "Charlie is the one who always lies."
- Bob: "Alice is not the one who alternates."
- Charlie: "Bob always tells the truth."
Determine who has which behavior (truth-teller, liar, alternator).
**Instructions:** 
- Reason step-by-step, evaluating the truthfulness of each statement under 
different assignments.
- Clearly state assumptions (e.g. "Assume Alice is the truth-teller, then ...") 
and check for contradictions.
- Conclude with a consistent assignment for Alice, Bob, Charlie and justify why 
it's correct.
Begin your logical analysis now.
```

This prompt sets up a classic logic puzzle and explicitly tells the model how to approach it: assume cases, check statements, find contradictions. The formatting (bullets with statements, separate instruction section) makes it clear. The chain-of-thought will involve trying different roles for Alice, Bob, Charlie and seeing which combination fits all statements. The prompt ensures the model does that systematically.

During the reasoning, if the model had a tool for solving CSPs (constraint satisfaction problems) it could use it, but more straightforward is just trial and error logically. The verification in this context is inherent: finding a contradiction means that assumption fails, etc. The final answer must be justified by pointing out that no statements are violated with that assignment – which the prompt's last bullet effectively asks for (justify why correct).

**State of the Art Note:** As mentioned, CoT is particularly effective for this kind of symbolic reasoning . We also have approaches like **self-consistency** (having the model try multiple reasoning paths). In a puzzle scenario, an agent could internally try solving with different methods or random starts to ensure it's not stuck in a biased path, then see which answer is most common. Prompt engineering can facilitate that by instructing multiple solution attempts, but that may be overkill unless the puzzle is extremely hard. [23](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=conducted%20a%20quantitative%20meta,CoT%27s%20gain%20comes%20from%20improving)

Also, *tool-augmented reasoning* like using an external solver (e.g. sending a logic formulation to a SAT solver via a plugin) is possible. The agent should choose that path if it's available and the problem is complex enough. In 2025, integration of LLMs with symbolic solvers is an active area, pointing toward hybrid reasoning systems . [25](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=symbolic%20operations%20and%20reasoning,whole%20range%20of%20LLM%20applications)

### **5. Complex File and System Operations**

**Challenges:** When the AI agent is effectively operating like a command-line user or system administrator (managing files, running server operations, etc.), the prompts must ensure the agent does not perform destructive actions without thought, and that it checks the system state before and after actions. The agent should also handle errors robustly (since mis-typed commands or unexpected environment issues can occur).

#### **Prompt Strategies:**

- **Plan actions with reasoning before execution:** Just as with other tasks, but especially in a system context, the agent should plan out what steps to take. A prompt to itself might be: *"List the sequence of shell commands or operations needed to achieve the goal, then execute them one by one."* This results in a safer, more deliberate approach rather than directly jumping into a rm -rf command, for example. The agent can be required to echo its plan: *"Proposed Plan: 1) do X, 2) do Y, 3) ... . Proceeding with Step 1."* This allows an oversight mechanism (if any) to catch a bad plan early, and also serves as a form of chain-of-thought. •
- **Query the environment for current state:** Prompt the agent to **verify assumptions** about the system before acting. E.g. *"Before creating a directory, check if it already exists."* This could be done by a ls or an if check. So the prompt might say: *"Always safely check if a file/directory exists before modifying or creating to avoid errors. When deleting, list the target to confirm it's correct."* By building such caution into the prompt, the agent will behave more like a careful sysadmin. •
- **Small, incremental steps:** Encourage the agent to perform operations step-by-step rather than a giant command that does everything at once. For instance, if asked to compress and archive logs, maybe do: list logs -> filter by date -> tar the files -> verify tar -> remove originals. The prompt can explicitly break it: *"Don't combine too many actions in one command. Do it stepwise so results can be checked."* This also aligns with easier debugging: if something goes wrong, it's clear at which step. •
- **Error handling and branching:** The agent should be ready to handle command failures. Prompt it with something like: *"If any command returns an error or unexpected output, pause and analyze the error. Decide whether to retry, skip, or adjust the plan."* For example, if cp fails because of permissions, the agent might need to use sudo or inform the user. The prompt can include guidelines: *"Never ignore errors; always address them or report them."* This ensures reliability and transparency. •
- **Post-action verification:** After executing commands, always verify the outcome. If the task was to modify a file, prompt the agent to open or cat the file to ensure the changes are present. If moving files, do a directory listing of the source and destination to confirm files moved correctly. The prompt line could be: *"After completing the operations, verify the final state matches the expected state (e.g., file exists in new location, old location is empty, etc.). If not, take corrective action or report the discrepancy."* This mirrors what the VerifyLLM approach does – analyzing the plan's result and adjusting . • [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites)
- **Safety and permissions:** Include any necessary cautions. If the environment has critical files, the agent should avoid dangerous commands unless absolutely sure. Perhaps the system prompt (a high-level instruction) includes something like: *"Never delete or modify files outside the specified scope. Confirm with a listing of target files to ensure you affect only intended files."* This can prevent accidents. •

#### **Example Prompt (File Management Task):**

User asks: *"Organize all log files in the /var/log/myapp/ directory that were last modified in July 2025 into a new subdirectory /var/log/myapp/archive/2025-07/ . Remove any duplicate files and provide a summary of what was archived."*

For the agent, an effective prompting and execution approach might be:

- **Plan (internal reasoning)**: The agent might think: need to find files modified in July 2025, ensure target directory exists, move files, avoid duplicates, then list moved files for summary. 1.
- **Verify preconditions**: Check that the source directory exists and is accessible, check if target dir exists (if not, plan to create it). 2.
- **Execute step by step**: 3.
- List files with ls -l or use find with date criteria (the agent may figure out a find command for modification date in July). 4.
- If duplicates (same name) are present, define what to do (maybe skip or rename? The prompt should clarify, or agent decides a policy). 5.
- Create archive directory structure. 6.
- Move files. 7.
- After moving, do ls in both source (to confirm files gone) and destination (to confirm files present). 8.
- **Summary**: The agent should then output a summary, e.g. "Archived X files from ... to ... . Listing of archived files: ..." 9.

A possible prompt to itself could be:

## Plan: 1. Find all files in /var/log/myapp/ last modified in July 2025. 2. Create directory /var/log/myapp/archive/2025-07/ if it doesn't exist. 3. For each file found: - If a file with the same name already exists in archive, decide to skip or rename (to avoid duplicates). - Move the file to the archive directory. 4. After moving, list the files in archive directory to confirm, and count them. 5. Output a summary of how many files were archived and list their names. Executing step 1: Finding files...

Then the agent would perform step 1 using the appropriate tool ( BashTool["find /var/log/myapp type f -newermt 2025-07-01 ! -newermt 2025-08-01"] for example). Observation returns list of files. Agent stores that, proceeds to step 2, etc. We see the principles: planned steps, verifying existence (via if [ ! -d ... ] then mkdir ... ), handling duplicates (maybe it checks if target file exists with -f and then appends a suffix to name or logs a skip), and verifying results.

Finally, it prints something like:

*"Archived 42 files into /var/log/myapp/archive/2025-07/. Files archived: file1.log, file2.log, ... file42.log."*

This final output is user-friendly and came from the prompt's request for a summary.

**References in Practice:** The above is consistent with how advanced agents (AutoGPT, etc.) operate in 2025 – they maintain an internal plan and have prompt templates guiding each action. The planning and verification loop aligns with the idea of pre-execution plan verification in robotics , just applied to file ops. By verifying after each step and adjusting, the agent prevents mistakes like missing some files or misplacing them. [82](https://arxiv.org/html/2507.05118v1#:~:text=In%20the%20field%20of%20robotics%2C,potential%20gaps%20in%20the%20plan) [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites)

## **Conclusion**

Instructing an AI agent to master prompt engineering in 2025 means teaching it to *think before it speaks (or acts)*, to leverage all available knowledge and tools, and to double-check its work as a diligent problem solver. We have covered how structured prompts with clear roles and contexts form the backbone for any complex task. Building on that, techniques like chain-of-thought prompting encourage deeper reasoning for multi-step problems, while advanced strategies such as tree-of-thought allow exploring multiple solution paths when needed. When an agent is endowed with tools and the ability to act, prompt patterns like ReAct become essential, interleaving reasoning with tool usage for factual correctness and expanded capabilities . [43](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=ReAct%20enables%20language%20models%20to,affect%20the%20internal%20state%20of) [45](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L324%20Wikipedia%20API,both%20internal%20knowledge%20and%20externally)

Equally important is the verification stage: methods like chain-of-verification reduce hallucinations by having the model fact-check itself , and Reflexion-style prompts push the agent to learn from its errors over iterations . Domain-specific considerations further tailor these approaches – from using unit tests in coding tasks to maintaining narrative coherence in storytelling. Throughout all domains, the prompts must foster a cycle of **plan → execute → verify** (and refine) . [51](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=information%E2%80%94%20in%20Large%20Language%20Models,reducing%20the%20occurrence%20of%20hallucinations) [53](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [58](https://www.promptingguide.ai/techniques/reflexion#:~:text=%2A%20Self,making) [56](https://www.promptingguide.ai/techniques/reflexion#:~:text=In%20summary%2C%20the%20key%20steps,reflection%20and%20memory%20components) 69 [5](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem)

As of August 2025, research and practice both indicate that prompt engineering is as much an art as a science, requiring empathy for how the model "thinks." An AI agent following the guidance in this paper will adopt a disciplined, methodical style of prompting itself and others, leading to outputs that are more accurate, reliable, and creative. By continuously staying updated – e.g., new prompt techniques or tools that emerge – the agent can further refine its skills. The meta-level ability to evaluate and improve one's own prompts ("prompting about prompting") is perhaps the final milestone in mastery . [83](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Meta) [66](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Original%20Prompt%3A%20)

In conclusion, **prompt engineering for complex multi-step operations** is the key to unlocking the full potential of AI agents. With robust prompts that plan actions, wield tools, and verify results, an AI agent can confidently tackle tasks that were once seen as too elaborate or open-ended. The instructions and examples provided here aim to serve as a comprehensive manual for such an agent – enabling it to not only solve complex problems but to do so with the expertise and finesse of a true "master prompt engineer."

The Ultimate Guide to Prompt Engineering in 2025 | Lakera – [1](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Prompt%20engineering%20isn%E2%80%99t%20just%20a,systems%20useful%2C%20reliable%2C%20and%20safe) [2](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=In%202023%2C%20you%20could%20get,assignments%2C%20and%20even%20adversarial%20exploits) [3](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=What%20Is%20Prompt%20Engineering%3F) [4](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=In%20simple%20terms%2C%20prompt%20engineering,a%20way%20it%20truly%20understands) [7](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=,thinner%20than%20most%20people%20think) [8](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=A%20Quick%20Example) [19](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=%2A%20Choose%20chain,analyst%2C%20or%20customer%20support%20agent) [20](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Combo%20Example%3A%20Role,thought) [22](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=match%20at%20L694%20Chain,intermediate%20steps%3A%20%E2%80%9CFirst%E2%80%A6%20then%E2%80%A6%20therefore%E2%80%A6%E2%80%9D) [26](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=Chain,intermediate%20steps%3A%20%E2%80%9CFirst%E2%80%A6%20then%E2%80%A6%20therefore%E2%80%A6%E2%80%9D) [81](https://www.lakera.ai/blog/prompt-engineering-guide#:~:text=%2A%20Use%20role,analyst%2C%20or%20customer%20support%20agent)

Protecting AI teams that disrupt the world.

<https://www.lakera.ai/blog/prompt-engineering-guide>

Next Gen LLM [5](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem) [9](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,these%20two%20requests) [10](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=When%20providing%20multiple%20instructions%2C%20remember,your%20instructions%20for%20best%20results) [11](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=to%20use%20its%20general%20knowledge,your%20provided%20information%20is%20incomplete) [12](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Managing%20Long%20Context%20Windows%3A%20Working,with%20Extensive%20Information) [13](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Strict%20Context%20Adherence%3A%20,your%20provided%20information%20is%20incomplete) [14](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Just%20as%20a%20well,understand%20and%20process%20your%20request) [15](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=When%20working%20with%20long%20prompts%2C,ensure%20they%20receive%20adequate%20attention) [16](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20is%20foundational%2C%20but%20worth,purpose%20to%20executing%20the%20task) [17](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20structure%20moves%20from%20context,to%20think%20about%20the%20task) [18](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=One%20of%20the%20most%20effective,it%20does%20to%20human%20students) [21](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Despite%20what%20you%20might%20have,work%20better%20than%20artificial%20emphasis) [36](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Query%20Decomposition%3A%20,together%20into%20a%20cohesive%20analysis) [37](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=%2A%20Synthesis%20and%20Integration%3A%20,together%20into%20a%20cohesive%20analysis) [38](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=to%20how%20we%20might%20teach,essay%20question%20into%20smaller%20components) [39](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=For%20complex%20problems%20requiring%20careful,essay%20question%20into%20smaller%20components) [40](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=This%20structured%20reasoning%20approach%20helps,clear%20sequence%20for%20addressing%20them) [46](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,after%20completing%20a%20math%20problem) [47](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=model%20from%20stopping%20halfway%20through,to%20plan%20an%20essay%20before) [48](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=,Implement%20planning%20before%20each) [49](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=reduces%20hallucination%20by%20encouraging%20the,after%20completing%20a%20math%20problem) [66](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Original%20Prompt%3A%20) [80](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=1,a%20clear%20call%20to%20action) [83](https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855#:~:text=Meta)

Prompting. A Guide for Practical Results | by Julian B | Aug, 2025 | Medium <https://medium.com/@julian.burns50/next-gen-llm-prompting-7b92f10f1855>

#### Chain-Of-VErification (COVE) Explained : r/PromptEngineering [6](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20Chain,reducing%20the%20occurrence%20of%20hallucinations) [51](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=information%E2%80%94%20in%20Large%20Language%20Models,reducing%20the%20occurrence%20of%20hallucinations) [52](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [53](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20efficacy%20of%20CoVe%20was,generated%20outputs) [54](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/#:~:text=The%20development%20and%20implementation%20of,more%20reliable%20and%20accurate%20LLMs)

[https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification\\_cove\\_explained/](https://www.reddit.com/r/PromptEngineering/comments/1bh610y/chainofverification_cove_explained/)

#### To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning | [23](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=conducted%20a%20quantitative%20meta,CoT%27s%20gain%20comes%20from%20improving) [24](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=CoT%20gives%20strong%20performance%20benefits,can%20be%20applied%20selectively%2C%20maintaining) [25](https://openreview.net/forum?id=w6nlcS8Kkn#:~:text=symbolic%20operations%20and%20reasoning,whole%20range%20of%20LLM%20applications)

#### OpenReview

<https://openreview.net/forum?id=w6nlcS8Kkn>

#### Tree of Thoughts (ToT) | Prompt Engineering Guide [27](https://www.promptingguide.ai/techniques/tot#:~:text=For%20complex%20tasks%20that%20require,problem%20solving%20with%20language%20models) [28](https://www.promptingguide.ai/techniques/tot#:~:text=Hulbert%20,A%20sample%20ToT%20prompt%20is) [29](https://www.promptingguide.ai/techniques/tot#:~:text=ToT%20maintains%20a%20tree%20of,thoughts%20with%20lookahead%20and%20backtracking) [30](https://www.promptingguide.ai/techniques/tot#:~:text=ToT%20maintains%20a%20tree%20of,thoughts%20with%20lookahead%20and%20backtracking) [31](https://www.promptingguide.ai/techniques/tot#:~:text=When%20using%20ToT%2C%20different%20tasks,best%20b%3D5%20candidates%20are%20kept) [32](https://www.promptingguide.ai/techniques/tot#:~:text=From%20the%20results%20reported%20in,outperforms%20the%20other%20prompting%20methods)

<https://www.promptingguide.ai/techniques/tot>

#### Papers | Prompt Engineering Guide [33](https://www.promptingguide.ai/papers#:~:text=%2A%20Zero,Retrieval%20Augmented%20Generation)

<https://www.promptingguide.ai/papers>

#### Advances in LLM Prompting and Model Capabilities: A 2024-2025 ... [34](https://www.reddit.com/r/PromptEngineering/comments/1ki9qwb/advances_in_llm_prompting_and_model_capabilities/#:~:text=5%20Advances%20in%20LLM%20Prompting,2025)

[https://www.reddit.com/r/PromptEngineering/comments/1ki9qwb/advances\\_in\\_llm\\_prompting\\_and\\_model\\_capabilities/](https://www.reddit.com/r/PromptEngineering/comments/1ki9qwb/advances_in_llm_prompting_and_model_capabilities/)

#### Chain of Targeted Verification Questions to Improve the Reliability of ... [35](https://2024.aiwareconf.org/details/aiware-2024-papers/16/Chain-of-Targeted-Verification-Questions-to-Improve-the-Reliability-of-Code-Generated#:~:text=Chain%20of%20Targeted%20Verification%20Questions,various%20nodes%20within%20the)

[https://2024.aiwareconf.org/details/aiware-2024-papers/16/Chain-of-Targeted-Verification-Questions-to-Improve-the-Reliability](https://2024.aiwareconf.org/details/aiware-2024-papers/16/Chain-of-Targeted-Verification-Questions-to-Improve-the-Reliability-of-Code-Generated)[of-Code-Generated](https://2024.aiwareconf.org/details/aiware-2024-papers/16/Chain-of-Targeted-Verification-Questions-to-Improve-the-Reliability-of-Code-Generated)

#### ReAct: Synergizing Reasoning and Acting in Language Models [41](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=In%20%E2%80%9CReAct%3A%20Synergizing%20Reasoning%20and,ReAct%29%20paradigm%20systematically) [42](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L264%20ReAct%20enables,affect%20the%20internal%20state%20of) [43](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=ReAct%20enables%20language%20models%20to,affect%20the%20internal%20state%20of) [44](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=Wikipedia%20API%20with%20which%20the,both%20internal%20knowledge%20and%20externally) [45](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=match%20at%20L324%20Wikipedia%20API,both%20internal%20knowledge%20and%20externally) [50](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/#:~:text=ReAct%20demonstrates%20the%20feasibility%20of,of%20research%20and%20leverage%20the)

<https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/>

#### Reflexion | Prompt Engineering Guide [55](https://www.promptingguide.ai/techniques/reflexion#:~:text=Reflexion%20is%20a%20framework%20to,a%20choice%20of%20LLM%20parameters) [56](https://www.promptingguide.ai/techniques/reflexion#:~:text=In%20summary%2C%20the%20key%20steps,reflection%20and%20memory%20components) [57](https://www.promptingguide.ai/techniques/reflexion#:~:text=At%20a%20high%20level%2C%20Reflexion,improvements%20on%20many%20advanced%20tasks) [58](https://www.promptingguide.ai/techniques/reflexion#:~:text=%2A%20Self,making) [59](https://www.promptingguide.ai/techniques/reflexion#:~:text=%2A%20Self,making) [60](https://www.promptingguide.ai/techniques/reflexion#:~:text=ReAct%20,relevant%20feedback%2C%20which%20is%20also) [61](https://www.promptingguide.ai/techniques/reflexion#:~:text=can%20learn%20to%20iteratively%20optimize,reflection%20and%20memory%20components) [62](https://www.promptingguide.ai/techniques/reflexion#:~:text=Experimental%20results%20demonstrate%20that%20Reflexion,Python%20programming%20tasks%20on%20HumanEval) [63](https://www.promptingguide.ai/techniques/reflexion#:~:text=Reflexion%20significantly%20outperforms%20all%20baseline,CoT%20with%20episodic%20memory%2C%20respectively) [65](https://www.promptingguide.ai/techniques/reflexion#:~:text=As%20summarized%20in%20the%20table,MBPP%2C%20HumanEval%2C%20and%20Leetcode%20Hard)

<https://www.promptingguide.ai/techniques/reflexion>

#### Reflexion: language agents with verbal reinforcement learning [64](https://nips.cc/virtual/2023/poster/70114#:~:text=Reflexion%3A%20language%20agents%20with%20verbal,buffer%20to%20induce%20better)

<https://nips.cc/virtual/2023/poster/70114>

#### [2405.13932] Chain of Targeted Verification Questions to Improve the Reliability of Code [67](https://arxiv.org/abs/2405.13932#:~:text=required%20by%20the%20user%20to,targeted%20VQs%20and%20the%20initial) [68](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,tasks%20in%20the%20CoderEval%20dataset) [69](https://arxiv.org/abs/2405.13932#:~:text=refinement%20method%20aimed%20at%20improving,targeted%20VQs%20and%20the%20initial) [70](https://arxiv.org/abs/2405.13932#:~:text=and%20in%20the%20absence%20of,tasks%20in%20the%20CoderEval%20dataset) [71](https://arxiv.org/abs/2405.13932#:~:text=target%20various%20nodes%20within%20the,executable%20code%20instances%20to%2013)

#### Generated by LLMs

<https://arxiv.org/abs/2405.13932>

#### Automating Test Driven Development with LLMs | by Benjamin | Medium [72](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=unit_test_system_prompt_string%20%3D%20,of%20the%20function%27s%20intended%20behavior) [73](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=2,Special%20characters) [74](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1#:~:text=expected%20behavior%2C%20including%3A%20,Special%20characters)

<https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1>

#### VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots <https://arxiv.org/html/2507.05118v1> [75](https://arxiv.org/html/2507.05118v1#:~:text=consists%20of%20two%20key%20steps%3A,io) [76](https://arxiv.org/html/2507.05118v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,action%20ordering%20and%20missing%20prerequisites) [77](https://arxiv.org/html/2507.05118v1#:~:text=Verifying%20robot%20action%20plans%20before,humans%20naturally%20take%20into%20account) [82](https://arxiv.org/html/2507.05118v1#:~:text=In%20the%20field%20of%20robotics%2C,potential%20gaps%20in%20the%20plan)

Reflexion: language agents with verbal reinforcement learning [78](https://openreview.net/forum?id=vAElhFcKW6#:~:text=learning%20openreview,for%20coding%20Reflexion%20has) [79](https://openreview.net/forum?id=vAElhFcKW6#:~:text=Debugging%20,for%20coding%20Reflexion%20has)

<https://openreview.net/forum?id=vAElhFcKW6>